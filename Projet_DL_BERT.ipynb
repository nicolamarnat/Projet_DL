{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_DL_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c4240a4943847fdb8570a41c84605e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ee05a2034f145a09b6111d3b78c286d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0529287fd23246e6a6b408c636e03816",
              "IPY_MODEL_119f913ca71649d7a8d4b38861caf705"
            ]
          }
        },
        "8ee05a2034f145a09b6111d3b78c286d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0529287fd23246e6a6b408c636e03816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c40a227f843342d1afc91638940743a1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bde3ecd9975e422388b8fb63c6a3fed4"
          }
        },
        "119f913ca71649d7a8d4b38861caf705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a789fdd45ae94986b04df4c94699e609",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 2.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4912d3f9543046b08c26fe96d1351083"
          }
        },
        "c40a227f843342d1afc91638940743a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bde3ecd9975e422388b8fb63c6a3fed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a789fdd45ae94986b04df4c94699e609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4912d3f9543046b08c26fe96d1351083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFNU-gzNIX0j"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgGbCe5eIZ4i"
      },
      "source": [
        "import torch\r\n",
        "   \r\n",
        "device = torch.device(\"cpu\")\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jy8e3UeG4Fz",
        "outputId": "b738be30-0477-42e2-fd61-babb1fd3322c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.9MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=a3367babae423ae17fecb6dc062fd919f36f2ebca3f1fb811d8f3418f9e9d7d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrMDSqlXIxzz",
        "outputId": "31d8a94c-7e14-491b-ebc2-e7bdd9372691"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=f5fcc2e8dfa156c0f1159ec8830cb2659324ec42b01c24f697e33d5020715a1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z7EMAx8GIMd"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPxy4TnQLTfO"
      },
      "source": [
        "# import wget\r\n",
        "# import os\r\n",
        "\r\n",
        "# # Download the dataset if we haven't already\r\n",
        "# url = 'https://github.com/nicolamarnat/Projet_DL/tree/master/dataset/dataset.zip'\r\n",
        "\r\n",
        "# if not os.path.exists('./dataset.zip'):\r\n",
        "#     print('Downloading dataset...')\r\n",
        "#     wget.download(url, './dataset.zip')\r\n",
        "    \r\n",
        "# # Unzip the dataset (if we haven't already)\r\n",
        "# if not os.path.exists('./dataset/'):\r\n",
        "#     print('Extracting the dataset files...')\r\n",
        "#     !unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26vn9ogRXmMh"
      },
      "source": [
        "On rÃ©cupÃ¨re localement le dataset contenant les relations leur label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuDO_hvPosND",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8e743278-b894-46ca-bbd2-2420b1636b20"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv(\"/content/relation_dataset_label.csv\", delimiter=';', header=None, names=['relation', 'label'])\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relation</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ville Paris comptait 2,187 millions habitants ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Paris joue rÃ´le plan histoire Europe monde</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Paris symbolise culture franÃ§aise</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rÃ©gion parisienne accueille institutions inter...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rÃ©gion parisienne une riches rÃ©gions Europe</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            relation  label\n",
              "0  ville Paris comptait 2,187 millions habitants ...      1\n",
              "1         Paris joue rÃ´le plan histoire Europe monde      0\n",
              "2                  Paris symbolise culture franÃ§aise      1\n",
              "3  rÃ©gion parisienne accueille institutions inter...      0\n",
              "4        rÃ©gion parisienne une riches rÃ©gions Europe      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbewmW2lGoGv"
      },
      "source": [
        "relations = df.relation.values\r\n",
        "labels = df.label.values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUcBOoDnG07R"
      },
      "source": [
        "# Bert tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXyJPu8KXtic"
      },
      "source": [
        "Avant d'effectuer l'entrainement, il nous faut convertir le dateset en tokens.\r\n",
        "\r\n",
        "Une partie du code provient de : https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7c4240a4943847fdb8570a41c84605e6",
            "8ee05a2034f145a09b6111d3b78c286d",
            "0529287fd23246e6a6b408c636e03816",
            "119f913ca71649d7a8d4b38861caf705",
            "c40a227f843342d1afc91638940743a1",
            "bde3ecd9975e422388b8fb63c6a3fed4",
            "a789fdd45ae94986b04df4c94699e609",
            "4912d3f9543046b08c26fe96d1351083"
          ]
        },
        "id": "tkswKJd9HD9f",
        "outputId": "f6538f6a-f0ec-4623-b3c8-14b7b0f91411"
      },
      "source": [
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "# Load the BERT tokenizer.\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c4240a4943847fdb8570a41c84605e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJGnXEbnHN9x",
        "outputId": "4f86e1e6-cc13-4803-94ff-c02aa3ac8da1"
      },
      "source": [
        "# Print the original sentence.\r\n",
        "print(' Original: ', relations[0])\r\n",
        "\r\n",
        "# Print the sentence split into tokens.\r\n",
        "print('Tokenized: ', tokenizer.tokenize(relations[0]))\r\n",
        "\r\n",
        "# Print the encoded sentence, ie., mapped to token ids.\r\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(relations[0])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  ville Paris comptait 2,187 millions habitants 1er janvier 2020\n",
            "Tokenized:  ['ville', 'Paris', 'comptait', '2', ',', '187', 'millions', 'habitants', '1er', 'janvier', '2020']\n",
            "Token IDs:  [11743, 10728, 28865, 123, 117, 23228, 18123, 12198, 99276, 13529, 23607]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQcILfCUYBS5"
      },
      "source": [
        "La mÃ©thode encode_plus nous permet de convertir le dataset directement au format attendu, en plaÃ§ant les tokens de dÃ©but/fin et de padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZV-_exAIKHW",
        "outputId": "89e5acaa-8ddb-4ab5-f034-4e18fae3eb9f"
      },
      "source": [
        "# Tokenize all of the relations and map the tokens to thier word IDs.\r\n",
        "input_ids = []\r\n",
        "attention_masks = []\r\n",
        "\r\n",
        "for rel in relations:\r\n",
        "    encoded_dict = tokenizer.encode_plus(rel, add_special_tokens = True, max_length = 64, truncation=True,\r\n",
        "                        pad_to_max_length = True, return_attention_mask = True, return_tensors = 'pt')\r\n",
        "    \r\n",
        "    input_ids.append(encoded_dict['input_ids'])\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "# Convert the lists into tensors.\r\n",
        "input_ids = torch.cat(input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "labels = torch.tensor(labels)\r\n",
        "\r\n",
        "print('Original: ', relations[0])\r\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ville Paris comptait 2,187 millions habitants 1er janvier 2020\n",
            "Token IDs: tensor([  101, 11743, 10728, 28865,   123,   117, 23228, 18123, 12198, 99276,\n",
            "        13529, 23607,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1uM3a4mNk1d"
      },
      "source": [
        "# Train and validation split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9KdbLESYOLF"
      },
      "source": [
        "SÃ©paration en un ensemble de train et un de validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTjSIqXyNon6",
        "outputId": "136b5863-9e33-4384-a803-4381e2582fed"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\r\n",
        "\r\n",
        "# Combine the training inputs into a TensorDataset.\r\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\r\n",
        "\r\n",
        "# Create a 90-10 train-validation split.\r\n",
        "train_size = int(0.9 * len(dataset))\r\n",
        "val_size = len(dataset) - train_size\r\n",
        "\r\n",
        "# Divide the dataset by randomly selecting samples.\r\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\r\n",
        "\r\n",
        "print('Training samples {}'.format(train_size))\r\n",
        "print('Validation samples {}'.format(val_size))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples 167\n",
            "Validation samples 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjSZi93dNz3H"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbOjO8SaYVwC"
      },
      "source": [
        "On utilise un DataLoader pour donner directment des batchs au rÃ©seau et ainsi optimiser la mÃ©moire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPh_94qCNvq3"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "batch_size = 40\r\n",
        "\r\n",
        "# Create the DataLoaders for our training and validation sets.\r\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset),\r\n",
        "            batch_size = batch_size)\r\n",
        "\r\n",
        "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset),\r\n",
        "            batch_size = batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHEhpcqYh0G"
      },
      "source": [
        "On charge un modÃ¨le prÃ©-entrainÃ© et on ajoute une couche non-entrainÃ©e. Nous allons entrainer la couche supplÃ©mentaire Ã  classer les relations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66X-sCEJN2gK"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained( \"bert-base-multilingual-cased\", num_labels = 2,\r\n",
        "    output_attentions = False, output_hidden_states = False)\r\n",
        "\r\n",
        "# Send model to GPU\r\n",
        "model.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmLC3DA3Y0Xx"
      },
      "source": [
        "HyperparamÃ¨tres : Adam avec Learning rate de 2e-5; 4 epochs; taille de batch 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE7YsLKYOB1s"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\r\n",
        "epochs = 4\r\n",
        "\r\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7o5i4glOFFW"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neaUsxyuV41N"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Set the seed for reproducibility\r\n",
        "seed_val = 42\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# To store training stats\r\n",
        "training_stats = []\r\n",
        "\r\n",
        "# Measure the total training time for the whole run.\r\n",
        "total_t0 = time.time()\r\n",
        "\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_train_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode.\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every 40 batches.\r\n",
        "        if step % 40 == 0 and not step == 0:\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # Unpack ther training batch and send them to GPU\r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "        # Clear Grads\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass \r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                             token_type_ids=None, \r\n",
        "                             attention_mask=b_input_mask, \r\n",
        "                             labels=b_labels)\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_train_loss = total_train_loss + outputs.loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass\r\n",
        "        outputs.loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss\r\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\r\n",
        "        \r\n",
        "    # Validation\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Put the model in evaluation mode\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    total_eval_accuracy = 0\r\n",
        "    total_eval_loss = 0\r\n",
        "    nb_eval_steps = 0\r\n",
        "\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "        \r\n",
        "        # No need to accumulate the grads\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            outputs = model(b_input_ids, \r\n",
        "                                   token_type_ids=None, \r\n",
        "                                   attention_mask=b_input_mask,\r\n",
        "                                   labels=b_labels)\r\n",
        "            \r\n",
        "        # Accumulate the validation loss.\r\n",
        "        total_eval_loss += outputs.loss.item()\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        logits = outputs.logits.detach().cpu().numpy()\r\n",
        "        label_ids = b_labels.to('cpu').numpy()\r\n",
        "\r\n",
        "        # Calculate the accuracy\r\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "        \r\n",
        "\r\n",
        "    # Report the final stats\r\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "    validation_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "    # Save stats\r\n",
        "    training_stats.append(\r\n",
        "        {\r\n",
        "            'epoch': epoch_i + 1,\r\n",
        "            'Training Loss': avg_train_loss,\r\n",
        "            'Valid. Loss': avg_val_loss,\r\n",
        "            'Valid. Accur.': avg_val_accuracy,\r\n",
        "            'Training Time': training_time,\r\n",
        "            'Validation Time': validation_time\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "print(\"Training complete!\")\r\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t9ZCLDfU0EM"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "# Use two decimal places.\r\n",
        "pd.set_option('precision', 2)\r\n",
        "# Create a DataFrame from our training statistics.\r\n",
        "df_stats = pd.DataFrame(data=training_stats)\r\n",
        "# Use the 'epoch' as the row index.\r\n",
        "df_stats = df_stats.set_index('epoch')\r\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgJKyKY1U2UK"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "% matplotlib inline\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Use plot styling from seaborn.\r\n",
        "sns.set(style='darkgrid')\r\n",
        "\r\n",
        "# Increase the plot size and font size.\r\n",
        "sns.set(font_scale=1.5)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\r\n",
        "\r\n",
        "# Plot the learning curve.\r\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\r\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\r\n",
        "plt.plot(df_stats['Valid. Accur.'], 'r-o', label=\"Validation Accuracy\")\r\n",
        "\r\n",
        "# Label the plot.\r\n",
        "plt.title(\"Training & Validation Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.xticks([1, 2, 3, 4])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vh0ub1HVkvH"
      },
      "source": [
        "# Lower batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVpZaUNxZNum"
      },
      "source": [
        "On passe Ã  une taille de batch de 10 pour essayer d'avoir de meilleurs rÃ©sultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YhvZWdQVscn"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "batch_size = 10\r\n",
        "\r\n",
        "# Create the DataLoaders for our training and validation sets.\r\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset),\r\n",
        "            batch_size = batch_size)\r\n",
        "\r\n",
        "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset),\r\n",
        "            batch_size = batch_size)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OqILHQfVyZ3"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained( \"bert-base-multilingual-cased\", num_labels = 2,\r\n",
        "    output_attentions = False, output_hidden_states = False)\r\n",
        "\r\n",
        "# Send model to GPU\r\n",
        "model.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdP92wKCV0gR"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\r\n",
        "epochs = 4\r\n",
        "\r\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq7YI-nEV2QF"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR0bOE1qOIhE"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Set the seed for reproducibility\r\n",
        "seed_val = 42\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# To store training stats\r\n",
        "training_stats = []\r\n",
        "\r\n",
        "# Measure the total training time for the whole run.\r\n",
        "total_t0 = time.time()\r\n",
        "\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_train_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode.\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every 40 batches.\r\n",
        "        if step % 40 == 0 and not step == 0:\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # Unpack ther training batch and send them to GPU\r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "        # Clear Grads\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass \r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                             token_type_ids=None, \r\n",
        "                             attention_mask=b_input_mask, \r\n",
        "                             labels=b_labels)\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_train_loss = total_train_loss + outputs.loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass\r\n",
        "        outputs.loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss\r\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\r\n",
        "        \r\n",
        "    # Validation\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Put the model in evaluation mode\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    total_eval_accuracy = 0\r\n",
        "    total_eval_loss = 0\r\n",
        "    nb_eval_steps = 0\r\n",
        "\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "        \r\n",
        "        # No need to accumulate the grads\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            outputs = model(b_input_ids, \r\n",
        "                                   token_type_ids=None, \r\n",
        "                                   attention_mask=b_input_mask,\r\n",
        "                                   labels=b_labels)\r\n",
        "            \r\n",
        "        # Accumulate the validation loss.\r\n",
        "        total_eval_loss += outputs.loss.item()\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        logits = outputs.logits.detach().cpu().numpy()\r\n",
        "        label_ids = b_labels.to('cpu').numpy()\r\n",
        "\r\n",
        "        # Calculate the accuracy\r\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "        \r\n",
        "\r\n",
        "    # Report the final stats\r\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "    validation_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "    # Save stats\r\n",
        "    training_stats.append(\r\n",
        "        {\r\n",
        "            'epoch': epoch_i + 1,\r\n",
        "            'Training Loss': avg_train_loss,\r\n",
        "            'Valid. Loss': avg_val_loss,\r\n",
        "            'Valid. Accur.': avg_val_accuracy,\r\n",
        "            'Training Time': training_time,\r\n",
        "            'Validation Time': validation_time\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "print(\"Training complete!\")\r\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVLRrCTZWCd0"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "# Use two decimal places.\r\n",
        "pd.set_option('precision', 2)\r\n",
        "# Create a DataFrame from our training statistics.\r\n",
        "df_stats = pd.DataFrame(data=training_stats)\r\n",
        "# Use the 'epoch' as the row index.\r\n",
        "df_stats = df_stats.set_index('epoch')\r\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzkbcypHWFVp"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "% matplotlib inline\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Use plot styling from seaborn.\r\n",
        "sns.set(style='darkgrid')\r\n",
        "\r\n",
        "# Increase the plot size and font size.\r\n",
        "sns.set(font_scale=1.5)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\r\n",
        "\r\n",
        "# Plot the learning curve.\r\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\r\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\r\n",
        "plt.plot(df_stats['Valid. Accur.'], 'g-o', label=\"Validation Accuracy\")\r\n",
        "\r\n",
        "# Label the plot.\r\n",
        "plt.title(\"Training & Validation Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.xticks([1, 2, 3, 4])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NojQiuf8s9mL"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJYll77DtAOL"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv(\"/content/test_dataset_label.csv\", delimiter=';', header=None, names=['relation', 'label'])\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv9h9Fk8tTOs"
      },
      "source": [
        "sentences = df.relation.values\r\n",
        "labels = df.label.values"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2faNbsMEvwlu"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\r\n",
        "\r\n",
        "input_ids = []\r\n",
        "attention_masks = []\r\n",
        "\r\n",
        "# We use the same max_lenght than before\r\n",
        "for sent in sentences:\r\n",
        "    encoded_dict = tokenizer.encode_plus(sent, add_special_tokens = True, max_length = 64, truncation=True,\r\n",
        "                        pad_to_max_length = True, return_attention_mask = True, return_tensors = 'pt')\r\n",
        "    \r\n",
        "    input_ids.append(encoded_dict['input_ids'])\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "# Convert the lists into tensors.\r\n",
        "input_ids = torch.cat(input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "labels = torch.tensor(labels)\r\n",
        "\r\n",
        "print('Original: ', sentences[0])\r\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruHtWjHmvxaP"
      },
      "source": [
        "# Create the Datasets & DataLoader.\r\n",
        "\r\n",
        "test_dataset = TensorDataset(input_ids, attention_masks, labels)\r\n",
        "\r\n",
        "batch_size = 10\r\n",
        "test_dataloader = DataLoader(test_dataset, sampler = RandomSampler(test_dataset),\r\n",
        "            batch_size = batch_size)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lr-xGJev3Ka"
      },
      "source": [
        "# Prediction on test set\r\n",
        "\r\n",
        "# Put model in evaluation mode\r\n",
        "model.eval()\r\n",
        "\r\n",
        "# Tracking variables, append the labels and prediction using these two lists\r\n",
        "predictions , true_labels = [], []\r\n",
        "\r\n",
        "for batch in test_dataloader:\r\n",
        "\r\n",
        "  b_input_ids = batch[0].to(device)\r\n",
        "  b_input_mask = batch[1].to(device)\r\n",
        "  b_labels = batch[2].to(device)\r\n",
        "  \r\n",
        "  with torch.no_grad():\r\n",
        "      # Forward pass, calculate logit predictions\r\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \r\n",
        "                      attention_mask=b_input_mask)\r\n",
        "\r\n",
        "  # Move logits and labels to CPU\r\n",
        "  logits = outputs.logits.detach().cpu().numpy()\r\n",
        "  label_ids = b_labels.to('cpu').numpy()\r\n",
        "  \r\n",
        "  # Store predictions and true labels\r\n",
        "  predictions.append(logits)\r\n",
        "  true_labels.append(label_ids)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhfMGSyQydIq"
      },
      "source": [
        "print(true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OFO7cTaykkB"
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z28JAyjxv4mX"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7pNxZRzZWrZ"
      },
      "source": [
        "On calcule le coefficient de corrÃ©lation de Matthews pour estimer la qualitÃ© de notre classificateur bianire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86PtMe6-xNVB"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\r\n",
        "\r\n",
        "matthews_set = []\r\n",
        "\r\n",
        "# Combine the results across all batches. \r\n",
        "flat_predictions = np.concatenate(predictions, axis=0)\r\n",
        "\r\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\r\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\r\n",
        "\r\n",
        "# Combine the correct labels for each batch into a single list.\r\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\r\n",
        "\r\n",
        "# Calculate the MCC\r\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\r\n",
        "\r\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}