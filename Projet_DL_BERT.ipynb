{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_DL_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c4240a4943847fdb8570a41c84605e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ee05a2034f145a09b6111d3b78c286d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0529287fd23246e6a6b408c636e03816",
              "IPY_MODEL_119f913ca71649d7a8d4b38861caf705"
            ]
          }
        },
        "8ee05a2034f145a09b6111d3b78c286d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0529287fd23246e6a6b408c636e03816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c40a227f843342d1afc91638940743a1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bde3ecd9975e422388b8fb63c6a3fed4"
          }
        },
        "119f913ca71649d7a8d4b38861caf705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a789fdd45ae94986b04df4c94699e609",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 2.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4912d3f9543046b08c26fe96d1351083"
          }
        },
        "c40a227f843342d1afc91638940743a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bde3ecd9975e422388b8fb63c6a3fed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a789fdd45ae94986b04df4c94699e609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4912d3f9543046b08c26fe96d1351083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFNU-gzNIX0j"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgGbCe5eIZ4i"
      },
      "source": [
        "import torch\r\n",
        "   \r\n",
        "device = torch.device(\"cpu\")\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jy8e3UeG4Fz",
        "outputId": "b738be30-0477-42e2-fd61-babb1fd3322c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=a3367babae423ae17fecb6dc062fd919f36f2ebca3f1fb811d8f3418f9e9d7d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrMDSqlXIxzz",
        "outputId": "31d8a94c-7e14-491b-ebc2-e7bdd9372691"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=f5fcc2e8dfa156c0f1159ec8830cb2659324ec42b01c24f697e33d5020715a1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z7EMAx8GIMd"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPxy4TnQLTfO"
      },
      "source": [
        "# import wget\r\n",
        "# import os\r\n",
        "\r\n",
        "# # Download the dataset if we haven't already\r\n",
        "# url = 'https://github.com/nicolamarnat/Projet_DL/tree/master/dataset/dataset.zip'\r\n",
        "\r\n",
        "# if not os.path.exists('./dataset.zip'):\r\n",
        "#     print('Downloading dataset...')\r\n",
        "#     wget.download(url, './dataset.zip')\r\n",
        "    \r\n",
        "# # Unzip the dataset (if we haven't already)\r\n",
        "# if not os.path.exists('./dataset/'):\r\n",
        "#     print('Extracting the dataset files...')\r\n",
        "#     !unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26vn9ogRXmMh"
      },
      "source": [
        "On récupère localement le dataset contenant les relations leur label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuDO_hvPosND",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8e743278-b894-46ca-bbd2-2420b1636b20"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv(\"/content/relation_dataset_label.csv\", delimiter=';', header=None, names=['relation', 'label'])\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relation</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ville Paris comptait 2,187 millions habitants ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Paris joue rôle plan histoire Europe monde</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Paris symbolise culture française</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>région parisienne accueille institutions inter...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>région parisienne une riches régions Europe</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            relation  label\n",
              "0  ville Paris comptait 2,187 millions habitants ...      1\n",
              "1         Paris joue rôle plan histoire Europe monde      0\n",
              "2                  Paris symbolise culture française      1\n",
              "3  région parisienne accueille institutions inter...      0\n",
              "4        région parisienne une riches régions Europe      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbewmW2lGoGv"
      },
      "source": [
        "relations = df.relation.values\r\n",
        "labels = df.label.values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUcBOoDnG07R"
      },
      "source": [
        "# Bert tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXyJPu8KXtic"
      },
      "source": [
        "Avant d'effectuer l'entrainement, il nous faut convertir le dateset en tokens.\r\n",
        "\r\n",
        "Une partie du code provient de : https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7c4240a4943847fdb8570a41c84605e6",
            "8ee05a2034f145a09b6111d3b78c286d",
            "0529287fd23246e6a6b408c636e03816",
            "119f913ca71649d7a8d4b38861caf705",
            "c40a227f843342d1afc91638940743a1",
            "bde3ecd9975e422388b8fb63c6a3fed4",
            "a789fdd45ae94986b04df4c94699e609",
            "4912d3f9543046b08c26fe96d1351083"
          ]
        },
        "id": "tkswKJd9HD9f",
        "outputId": "f6538f6a-f0ec-4623-b3c8-14b7b0f91411"
      },
      "source": [
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "# Load the BERT tokenizer.\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c4240a4943847fdb8570a41c84605e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJGnXEbnHN9x",
        "outputId": "4f86e1e6-cc13-4803-94ff-c02aa3ac8da1"
      },
      "source": [
        "# Print the original sentence.\r\n",
        "print(' Original: ', relations[0])\r\n",
        "\r\n",
        "# Print the sentence split into tokens.\r\n",
        "print('Tokenized: ', tokenizer.tokenize(relations[0]))\r\n",
        "\r\n",
        "# Print the encoded sentence, ie., mapped to token ids.\r\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(relations[0])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  ville Paris comptait 2,187 millions habitants 1er janvier 2020\n",
            "Tokenized:  ['ville', 'Paris', 'comptait', '2', ',', '187', 'millions', 'habitants', '1er', 'janvier', '2020']\n",
            "Token IDs:  [11743, 10728, 28865, 123, 117, 23228, 18123, 12198, 99276, 13529, 23607]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQcILfCUYBS5"
      },
      "source": [
        "La méthode encode_plus nous permet de convertir le dataset directement au format attendu, en plaçant les tokens de début/fin et de padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZV-_exAIKHW",
        "outputId": "89e5acaa-8ddb-4ab5-f034-4e18fae3eb9f"
      },
      "source": [
        "# Tokenize all of the relations and map the tokens to thier word IDs.\r\n",
        "input_ids = []\r\n",
        "attention_masks = []\r\n",
        "\r\n",
        "for rel in relations:\r\n",
        "    encoded_dict = tokenizer.encode_plus(rel, add_special_tokens = True, max_length = 64, truncation=True,\r\n",
        "                        pad_to_max_length = True, return_attention_mask = True, return_tensors = 'pt')\r\n",
        "    \r\n",
        "    input_ids.append(encoded_dict['input_ids'])\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "# Convert the lists into tensors.\r\n",
        "input_ids = torch.cat(input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "labels = torch.tensor(labels)\r\n",
        "\r\n",
        "print('Original: ', relations[0])\r\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ville Paris comptait 2,187 millions habitants 1er janvier 2020\n",
            "Token IDs: tensor([  101, 11743, 10728, 28865,   123,   117, 23228, 18123, 12198, 99276,\n",
            "        13529, 23607,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1uM3a4mNk1d"
      },
      "source": [
        "# Train and validation split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9KdbLESYOLF"
      },
      "source": [
        "Séparation en un ensemble de train et un de validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTjSIqXyNon6",
        "outputId": "136b5863-9e33-4384-a803-4381e2582fed"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\r\n",
        "\r\n",
        "# Combine the training inputs into a TensorDataset.\r\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\r\n",
        "\r\n",
        "# Create a 90-10 train-validation split.\r\n",
        "train_size = int(0.9 * len(dataset))\r\n",
        "val_size = len(dataset) - train_size\r\n",
        "\r\n",
        "# Divide the dataset by randomly selecting samples.\r\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\r\n",
        "\r\n",
        "print('Training samples {}'.format(train_size))\r\n",
        "print('Validation samples {}'.format(val_size))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples 167\n",
            "Validation samples 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjSZi93dNz3H"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbOjO8SaYVwC"
      },
      "source": [
        "On utilise un DataLoader pour donner directment des batchs au réseau et ainsi optimiser la mémoire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPh_94qCNvq3"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "batch_size = 40\r\n",
        "\r\n",
        "# Create the DataLoaders for our training and validation sets.\r\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset),\r\n",
        "            batch_size = batch_size)\r\n",
        "\r\n",
        "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset),\r\n",
        "            batch_size = batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHEhpcqYh0G"
      },
      "source": [
        "On charge un modèle pré-entrainé et on ajoute une couche non-entrainée. Nous allons entrainer la couche supplémentaire à classer les relations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66X-sCEJN2gK"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained( \"bert-base-multilingual-cased\", num_labels = 2,\r\n",
        "    output_attentions = False, output_hidden_states = False)\r\n",
        "\r\n",
        "# Send model to GPU\r\n",
        "model.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmLC3DA3Y0Xx"
      },
      "source": [
        "Hyperparamètres : Adam avec Learning rate de 2e-5; 4 epochs; taille de batch 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE7YsLKYOB1s"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\r\n",
        "epochs = 4\r\n",
        "\r\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7o5i4glOFFW"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neaUsxyuV41N"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Set the seed for reproducibility\r\n",
        "seed_val = 42\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# To store training stats\r\n",
        "training_stats = []\r\n",
        "\r\n",
        "# Measure the total training time for the whole run.\r\n",
        "total_t0 = time.time()\r\n",
        "\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_train_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode.\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every 40 batches.\r\n",
        "        if step % 40 == 0 and not step == 0:\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # Unpack ther training batch and send them to GPU\r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "        # Clear Grads\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass \r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                             token_type_ids=None, \r\n",
        "                             attention_mask=b_input_mask, \r\n",
        "                             labels=b_labels)\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_train_loss = total_train_loss + outputs.loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass\r\n",
        "        outputs.loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss\r\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\r\n",
        "        \r\n",
        "    # Validation\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Put the model in evaluation mode\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    total_eval_accuracy = 0\r\n",
        "    total_eval_loss = 0\r\n",
        "    nb_eval_steps = 0\r\n",
        "\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "        \r\n",
        "        # No need to accumulate the grads\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            outputs = model(b_input_ids, \r\n",
        "                                   token_type_ids=None, \r\n",
        "                                   attention_mask=b_input_mask,\r\n",
        "                                   labels=b_labels)\r\n",
        "            \r\n",
        "        # Accumulate the validation loss.\r\n",
        "        total_eval_loss += outputs.loss.item()\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        logits = outputs.logits.detach().cpu().numpy()\r\n",
        "        label_ids = b_labels.to('cpu').numpy()\r\n",
        "\r\n",
        "        # Calculate the accuracy\r\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "        \r\n",
        "\r\n",
        "    # Report the final stats\r\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "    validation_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "    # Save stats\r\n",
        "    training_stats.append(\r\n",
        "        {\r\n",
        "            'epoch': epoch_i + 1,\r\n",
        "            'Training Loss': avg_train_loss,\r\n",
        "            'Valid. Loss': avg_val_loss,\r\n",
        "            'Valid. Accur.': avg_val_accuracy,\r\n",
        "            'Training Time': training_time,\r\n",
        "            'Validation Time': validation_time\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "print(\"Training complete!\")\r\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t9ZCLDfU0EM"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "# Use two decimal places.\r\n",
        "pd.set_option('precision', 2)\r\n",
        "# Create a DataFrame from our training statistics.\r\n",
        "df_stats = pd.DataFrame(data=training_stats)\r\n",
        "# Use the 'epoch' as the row index.\r\n",
        "df_stats = df_stats.set_index('epoch')\r\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgJKyKY1U2UK"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "% matplotlib inline\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Use plot styling from seaborn.\r\n",
        "sns.set(style='darkgrid')\r\n",
        "\r\n",
        "# Increase the plot size and font size.\r\n",
        "sns.set(font_scale=1.5)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\r\n",
        "\r\n",
        "# Plot the learning curve.\r\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\r\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\r\n",
        "plt.plot(df_stats['Valid. Accur.'], 'r-o', label=\"Validation Accuracy\")\r\n",
        "\r\n",
        "# Label the plot.\r\n",
        "plt.title(\"Training & Validation Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.xticks([1, 2, 3, 4])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vh0ub1HVkvH"
      },
      "source": [
        "# Lower batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVpZaUNxZNum"
      },
      "source": [
        "On passe à une taille de batch de 10 pour essayer d'avoir de meilleurs résultats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YhvZWdQVscn"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "batch_size = 10\r\n",
        "\r\n",
        "# Create the DataLoaders for our training and validation sets.\r\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset),\r\n",
        "            batch_size = batch_size)\r\n",
        "\r\n",
        "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset),\r\n",
        "            batch_size = batch_size)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OqILHQfVyZ3"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained( \"bert-base-multilingual-cased\", num_labels = 2,\r\n",
        "    output_attentions = False, output_hidden_states = False)\r\n",
        "\r\n",
        "# Send model to GPU\r\n",
        "model.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdP92wKCV0gR"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\r\n",
        "epochs = 4\r\n",
        "\r\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq7YI-nEV2QF"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR0bOE1qOIhE"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Set the seed for reproducibility\r\n",
        "seed_val = 42\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# To store training stats\r\n",
        "training_stats = []\r\n",
        "\r\n",
        "# Measure the total training time for the whole run.\r\n",
        "total_t0 = time.time()\r\n",
        "\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_train_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode.\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every 40 batches.\r\n",
        "        if step % 40 == 0 and not step == 0:\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # Unpack ther training batch and send them to GPU\r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "        # Clear Grads\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass \r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                             token_type_ids=None, \r\n",
        "                             attention_mask=b_input_mask, \r\n",
        "                             labels=b_labels)\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_train_loss = total_train_loss + outputs.loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass\r\n",
        "        outputs.loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss\r\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    training_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\r\n",
        "        \r\n",
        "    # Validation\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Put the model in evaluation mode\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    total_eval_accuracy = 0\r\n",
        "    total_eval_loss = 0\r\n",
        "    nb_eval_steps = 0\r\n",
        "\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "        \r\n",
        "        # No need to accumulate the grads\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            outputs = model(b_input_ids, \r\n",
        "                                   token_type_ids=None, \r\n",
        "                                   attention_mask=b_input_mask,\r\n",
        "                                   labels=b_labels)\r\n",
        "            \r\n",
        "        # Accumulate the validation loss.\r\n",
        "        total_eval_loss += outputs.loss.item()\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        logits = outputs.logits.detach().cpu().numpy()\r\n",
        "        label_ids = b_labels.to('cpu').numpy()\r\n",
        "\r\n",
        "        # Calculate the accuracy\r\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\r\n",
        "        \r\n",
        "\r\n",
        "    # Report the final stats\r\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\r\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\r\n",
        "    validation_time = format_time(time.time() - t0)\r\n",
        "\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\r\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\r\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\r\n",
        "\r\n",
        "    # Save stats\r\n",
        "    training_stats.append(\r\n",
        "        {\r\n",
        "            'epoch': epoch_i + 1,\r\n",
        "            'Training Loss': avg_train_loss,\r\n",
        "            'Valid. Loss': avg_val_loss,\r\n",
        "            'Valid. Accur.': avg_val_accuracy,\r\n",
        "            'Training Time': training_time,\r\n",
        "            'Validation Time': validation_time\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "print(\"Training complete!\")\r\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVLRrCTZWCd0"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "# Use two decimal places.\r\n",
        "pd.set_option('precision', 2)\r\n",
        "# Create a DataFrame from our training statistics.\r\n",
        "df_stats = pd.DataFrame(data=training_stats)\r\n",
        "# Use the 'epoch' as the row index.\r\n",
        "df_stats = df_stats.set_index('epoch')\r\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzkbcypHWFVp"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "% matplotlib inline\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Use plot styling from seaborn.\r\n",
        "sns.set(style='darkgrid')\r\n",
        "\r\n",
        "# Increase the plot size and font size.\r\n",
        "sns.set(font_scale=1.5)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\r\n",
        "\r\n",
        "# Plot the learning curve.\r\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\r\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\r\n",
        "plt.plot(df_stats['Valid. Accur.'], 'g-o', label=\"Validation Accuracy\")\r\n",
        "\r\n",
        "# Label the plot.\r\n",
        "plt.title(\"Training & Validation Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "plt.legend()\r\n",
        "plt.xticks([1, 2, 3, 4])\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NojQiuf8s9mL"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJYll77DtAOL"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df = pd.read_csv(\"/content/test_dataset_label.csv\", delimiter=';', header=None, names=['relation', 'label'])\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv9h9Fk8tTOs"
      },
      "source": [
        "sentences = df.relation.values\r\n",
        "labels = df.label.values"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2faNbsMEvwlu"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\r\n",
        "\r\n",
        "input_ids = []\r\n",
        "attention_masks = []\r\n",
        "\r\n",
        "# We use the same max_lenght than before\r\n",
        "for sent in sentences:\r\n",
        "    encoded_dict = tokenizer.encode_plus(sent, add_special_tokens = True, max_length = 64, truncation=True,\r\n",
        "                        pad_to_max_length = True, return_attention_mask = True, return_tensors = 'pt')\r\n",
        "    \r\n",
        "    input_ids.append(encoded_dict['input_ids'])\r\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\r\n",
        "\r\n",
        "# Convert the lists into tensors.\r\n",
        "input_ids = torch.cat(input_ids, dim=0)\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\r\n",
        "labels = torch.tensor(labels)\r\n",
        "\r\n",
        "print('Original: ', sentences[0])\r\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruHtWjHmvxaP"
      },
      "source": [
        "# Create the Datasets & DataLoader.\r\n",
        "\r\n",
        "test_dataset = TensorDataset(input_ids, attention_masks, labels)\r\n",
        "\r\n",
        "batch_size = 10\r\n",
        "test_dataloader = DataLoader(test_dataset, sampler = RandomSampler(test_dataset),\r\n",
        "            batch_size = batch_size)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lr-xGJev3Ka"
      },
      "source": [
        "# Prediction on test set\r\n",
        "\r\n",
        "# Put model in evaluation mode\r\n",
        "model.eval()\r\n",
        "\r\n",
        "# Tracking variables, append the labels and prediction using these two lists\r\n",
        "predictions , true_labels = [], []\r\n",
        "\r\n",
        "for batch in test_dataloader:\r\n",
        "\r\n",
        "  b_input_ids = batch[0].to(device)\r\n",
        "  b_input_mask = batch[1].to(device)\r\n",
        "  b_labels = batch[2].to(device)\r\n",
        "  \r\n",
        "  with torch.no_grad():\r\n",
        "      # Forward pass, calculate logit predictions\r\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \r\n",
        "                      attention_mask=b_input_mask)\r\n",
        "\r\n",
        "  # Move logits and labels to CPU\r\n",
        "  logits = outputs.logits.detach().cpu().numpy()\r\n",
        "  label_ids = b_labels.to('cpu').numpy()\r\n",
        "  \r\n",
        "  # Store predictions and true labels\r\n",
        "  predictions.append(logits)\r\n",
        "  true_labels.append(label_ids)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhfMGSyQydIq"
      },
      "source": [
        "print(true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OFO7cTaykkB"
      },
      "source": [
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z28JAyjxv4mX"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7pNxZRzZWrZ"
      },
      "source": [
        "On calcule le coefficient de corrélation de Matthews pour estimer la qualité de notre classificateur bianire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86PtMe6-xNVB"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\r\n",
        "\r\n",
        "matthews_set = []\r\n",
        "\r\n",
        "# Combine the results across all batches. \r\n",
        "flat_predictions = np.concatenate(predictions, axis=0)\r\n",
        "\r\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\r\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\r\n",
        "\r\n",
        "# Combine the correct labels for each batch into a single list.\r\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\r\n",
        "\r\n",
        "# Calculate the MCC\r\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\r\n",
        "\r\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}